{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Basic Chatbot\n",
        "\n",
        "Before we dive into the exciting world of bringing our very own ChatBot, we will want to review a few concepts and expand our knowledge on how to perform API calls, with the concept of **\"Message Threading\"**\n",
        "\n",
        "\n",
        "## How Message Threading Works\n",
        "Message threading is a conversation technique that allows multiple participants to communicate clearly within a linked series of messages. Here is an overview of how threaded messaging facilitates an organized dialogue:\n",
        "\n",
        "## Participants\n",
        "There are three main roles that contribute to a message thread conversation: System, User, and Assistant\n",
        "\n",
        "\n",
        "\n",
        "### The System\n",
        "Initiates the conversation by providing opening instructions and prompts.\n",
        "Guides the dialogue on the back-end by providing your instructions, which helps \"mold\" responses from the Assistant\n",
        "\n",
        "For example:\n",
        "\n",
        "**System**: You are a helpful assistant.\n",
        "\n",
        "**System**: You are a geography expert.\n",
        "\n",
        "### The User\n",
        "Drives the conversation based on prompts from the system.\n",
        "Asks questions, provides responses, and makes follow-up comments.\n",
        "\n",
        "For example:\n",
        "\n",
        "**User**: What is your name?\n",
        "\n",
        "**User**: What is the capital of the United States?\n",
        "\n",
        "The Assistant\n",
        "Responds directly to each of the user's messages.\n",
        "Answers questions and provides relevant information.\n",
        "Continues the conversation with the user naturally.\n",
        "\n",
        "For example:\n",
        "\n",
        "**Assistant**: I am ChatGPT developed by OpenAI, I am a helpful assistant here to answer your questions!\n",
        "\n",
        "**Assistant**: The capital of the United States of America is Washington D.C. ...\n"
      ],
      "metadata": {
        "id": "mcMLLWlUOWNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDB0VAL-OSlP",
        "outputId": "4a1268e4-ee84-440e-f03f-13c68f22482c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.2.4-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.2/220.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayi2cUzXOLCs"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo-1106\"\n",
        "#model = \"gpt-4-1106-preview\"\n",
        "openai.api_key = \"YOUR API KEY\"\n"
      ],
      "metadata": {
        "id": "ns-Ao1wlPgXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I output all files in a directory using Python?\",\n",
        "        },\n",
        "    ],\n",
        ")\n"
      ],
      "metadata": {
        "id": "CSQbtOElUCOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU TRY:\n",
        "\n",
        "Looks like we'll need to parse out the response. How can we do this in a way that helps us get the actual content of the message, without all the extra \"metadata\"?"
      ],
      "metadata": {
        "id": "v9kZI_PcQatr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3O-nyalHQaJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU TRY:\n",
        "\n",
        "Now let's write a simple function that loads our model type, and our prompt, to generate a working output directly, so we can pass that function directly! We will be using the GPT 3.5 model (Turbo) which is specified as the parameter: `model = \"gpt-3.5-turbo-1106\"`"
      ],
      "metadata": {
        "id": "cH3vNPuRQ7ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MESSAGES = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I output all files in a directory using Python?\",\n",
        "        },\n",
        "    ]"
      ],
      "metadata": {
        "id": "6XTgvE-9RoKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "4mCUMkSfQ68U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expected Response\n",
        "You can use the `os` module in Python to achieve this. Here's an example code to output all files in a directory:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "directory = '/path/to/your/directory'\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if os.path.isfile(os.path.join(directory, filename)):\n",
        "        print(filename)\n",
        "```\n",
        "\n",
        "Replace `'/path/to/your/directory'` with the actual path of the directory you want to list the files from. This code will iterate through all the files in the directory and print their names."
      ],
      "metadata": {
        "id": "sEHoKxZpXmRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try another example"
      ],
      "metadata": {
        "id": "nONMZQQVYAax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MESSAGES2=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who knows how to program.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "r67u_J9sX_Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_response(MESSAGES,model = \"gpt-3.5-turbo-1106\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3Ycv61yYDBM",
        "outputId": "6164df40-573e-44cf-a0f7-af253573422a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The World Series in 2020 was played at Globe Life Field in Arlington, Texas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Latest and Greatest\n",
        "\n",
        "At the time of this writing, the newest model that has been released is GPT4-turbo which as an advanced version of GPT-4 which has the following key features:\n",
        "\n",
        "- 128K context window\n",
        "\n",
        "- 0.3 US cents per 1000 tokens\n",
        "\n",
        "- Faster than GPT-4\n",
        "\n",
        "- Has a \"Vision\" model to see images\n",
        "\n",
        "In order to test out the GPT4-turbo (text) model, we just have to change the model flag in our `get_response()` function to be:\n",
        "\n",
        "`model = \"gpt-4-1106-preview\"`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f3bwuWe-OLqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOU TRY\n",
        "\n",
        "Try this out now with while trying to help write an email to your boss explaining why we need to consider why we need to push the deadline back. Try out the responses from both the gpt3.5 model and new gpt-4 model.\n",
        "\n"
      ],
      "metadata": {
        "id": "bsFeZOtiYm8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MESSAGES = [...]\n",
        "print(\"GPT 3.5 Response:\")\n",
        "\n",
        "# YOUR CODE HERE\n",
        "print(\"GPT 4 Turbo Response: \")\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "LLnvmcS3ZBr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Flow\n",
        "The conversation flows smoothly through the interactions between the different roles:\n",
        "\n",
        "The **system** provides an opening prompt to initiate the dialogue.\n",
        "\n",
        "The **user** responds to the prompt, posing their first question or comment to the assistant. This starts a message thread.\n",
        "\n",
        "The **assistant** directly answers the user's message, keeping their response in the same thread.\n",
        "\n",
        "The **user** can then follow up with additional questions or comments related to the assistant's response, further continuing the thread.\n",
        "\n",
        "This **cycle repeats**, with the assistant replying to each of the user's messages to maintain a natural back-and-forth discussion.\n",
        "\n",
        "When needed, the **system** can provide another prompt to start a new thread on a different topic.\n",
        "Message Organization\n",
        "Threading keeps the various messages organized:\n",
        "\n",
        "Messages that are part of the same thread are grouped visually so they are easy to follow.\n",
        "Each thread stems from an initial prompt and contains a chain of relevant user and assistant replies.\n",
        "Shifting to a new prompt begins a new thread with its own distinct message chain separate from previous threads.\n",
        "This structure allows multiple threads to exist simultaneously without confusion."
      ],
      "metadata": {
        "id": "ZH9f-dAmihff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU TRY\n",
        "### System Messages\n",
        "This time, let's try the same prompt again, but we'll add a \"system messagee\" that will alow us to provide some more detailed instructions like:\n",
        "\"\"You are a wise negotiator and business leader who writes expert-level persuasive emails\""
      ],
      "metadata": {
        "id": "ZQuGhglIaFgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've done a couple of more detailed examples on how to access the OpenAI API with some of the latest models, we can try to think about how we would help make some more abstractions in order to help run this as a chatbot.\n",
        "\n",
        "Take a moment to think about it and reflect.\n",
        "\n",
        "### Reflect: What will we need to encode a chatbot that goes back and forth, that maintains context over time?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w_9hWNm6YVAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Reflections and ideas here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HxMphHtebIZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we want to do is probably think of the general conversation threading and think about \"actions\" for each of the THREE PLAYERS: System, User, Assistant.\n",
        "\n",
        "How can we do this?\n",
        "\n",
        "Well, as often happens in the world of coding...we can actually ask ChatGPT for some advice and work from there."
      ],
      "metadata": {
        "id": "zR8lqCGcbDrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "code_sample = \"\"\"MESSAGES = [{\"role\":\"system\",\n",
        "             \"content\":\"You are a wise negotiator and business leader who writes expert level persuasive emails\"},\n",
        "    {\"role\":\"user\",\n",
        "     \"content\": \"Help me write a cordial and concise email to my boss explaining why we need to push back the product launch.\"}]\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are a coding assistant and Python expert. I am working with a Generative AI API, and I would like to\n",
        "figure out how I can create an automated loop chatbot, that uses only the simplest tools possible\n",
        "to help extend a message thread that is shown below separated by 3 backticks.\n",
        "```python\n",
        "{code_sample}\n",
        "```\n",
        "\"\"\"\n",
        "MESSAGES = [{\"role\":\"system\",\n",
        "             \"content\":\"You are a programming expert\"},\n",
        "    {\"role\":\"user\",\n",
        "     \"content\": prompt}]\n",
        "# print(\"GPT 3.5 Response:\")\n",
        "\n",
        "# print(get_response(MESSAGES,model = \"gpt-3.5-turbo-1106\"))\n",
        "\n",
        "print(\"GPT 4 Turbo Response: \")\n",
        "print(get_response(MESSAGES,model = \"gpt-4-1106-preview\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXlQ33LYauRj",
        "outputId": "8280aad4-4921-488a-eb31-80f515b1d625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT 4 Turbo Response: \n",
            "Certainly, you can create a simple loop chatbot in Python that uses this Generative AI API to generate responses. I'll provide you with a basic structure of the code. In this example, I will assume that the API interaction is conducted via a function `generate_response`, which you'll have to replace with the actual API call method provided by your Generative AI service.\n",
            "\n",
            "Here's a simple example of a loop chatbot in Python:\n",
            "```python\n",
            "import json\n",
            "import requests\n",
            "\n",
            "# This is the function you'll need to replace with the actual API call.\n",
            "# It should take the context and return the AI's response.\n",
            "def generate_response(context):\n",
            "    # Simulating an API call with a static response for illustration purposes.\n",
            "    return \"This is a simulated response.\"\n",
            "\n",
            "# Initialize message thread.\n",
            "MESSAGES = [\n",
            "    {\"role\": \"system\",\n",
            "     \"content\": \"You are a wise negotiator and business leader who writes expert level persuasive emails\"},\n",
            "    {\"role\": \"user\",\n",
            "     \"content\": \"Help me write a cordial and concise email to my boss explaining why we need to push back the product launch.\"}\n",
            "]\n",
            "\n",
            "# Begin conversation loop.\n",
            "while True:\n",
            "    chat_input = input(\"You: \")\n",
            "    \n",
            "    if chat_input.lower() == \"quit\":\n",
            "        print(\"Exiting chat.\")\n",
            "        break\n",
            "    \n",
            "    # Append user input to messages.\n",
            "    MESSAGES.append({\"role\": \"user\", \"content\": chat_input})\n",
            "    \n",
            "    # Create a context from the messages.\n",
            "    context = \"\"\n",
            "    for message in MESSAGES:\n",
            "        role = message[\"role\"]\n",
            "        content = message[\"content\"]\n",
            "        context += f'{role}: {content}\\n'\n",
            "\n",
            "    # Call the generate_response function (or API call) with the context.\n",
            "    ai_response = generate_response(context)\n",
            "\n",
            "    # Append the AI's response to the messages.\n",
            "    MESSAGES.append({\"role\": \"ai\", \"content\": ai_response})\n",
            "    \n",
            "    print(\"AI:\", ai_response)\n",
            "\n",
            "# Exit the program.\n",
            "print(\"Chatbot session has ended.\")\n",
            "```\n",
            "\n",
            "Some important things to consider:\n",
            "- The `generate_response` function is a placeholder. You must implement it to include the specific instructions for interacting with your AI API, this includes handling authentication, headers, body, and making a `POST` request, interpreting the response, and more.\n",
            "- If the API requires a more complex structure of the input (like tokens, ids, etc.), you need to modify the `context` variable accordingly.\n",
            "- The loop continues until the user types `quit`. Depending on your use case, you might want to implement additional commands for the user to interact with the bot (like restarting the conversation, asking for AI help outside the context, etc.).\n",
            "- When the loop ends, the program will print a message indicating the session has ended, but you can also add some cleanup actions if necessary for your specific setup before the program exits.\n",
            "- Be sure to handle errors and timeouts that might occur during the API request robustly, this code doesn't include error handling for simplicity.\n",
            "\n",
            "Remember to replace the placeholder function with actual code that sends and receives data from the Generative AI API you are using. Each API provider typically has documentation with examples that you can follow to make sure you're forming your requests properly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU TRY\n",
        "\n",
        "Try out some of the suggestions, if there is any valuable information presented, in order to think through the problem of how we might actually get a working chat thread that has the following requirements:\n",
        "\n",
        "- uses an initial input from the user\n",
        "- system instructions have a certain intent (if desired), but can be included in the MESSAGES variable directly\n",
        "- the conversation maintains the history and brings subsequent results based on the entire context history\n",
        "- continues the chat until the user says \"quit\"\n",
        "\n",
        "**HINT: it is probably not as complicated as the AI guessed, and you should focus on the principles required to build the loop rather than the cut-and-dry code presented by ChatGPT.**\n",
        "\n",
        "**HINT2: ChatGPT does not know its own documentation (and recent updates) in its training...probably.**"
      ],
      "metadata": {
        "id": "rv1t3IRhcgpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR SOLUTION HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uOMBW6GgcL_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You've Done it!\n",
        "\n",
        "There you go, you have now built a simple chat bot that:\n",
        "- Uses OpenAI API calls (with the latest updates)\n",
        "\n",
        "- Uses instruction tuned prompts as well as User prompts\n",
        "\n",
        "- Remembers CONTEXT in its responses"
      ],
      "metadata": {
        "id": "Z2HdshCMfxpo"
      }
    }
  ]
}