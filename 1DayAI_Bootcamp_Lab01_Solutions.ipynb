{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hands-On Activity: Generating Text with ChatGPT (API)\n",
        "\n",
        "In this lab, we will explore the capabilities of ChatGPT by implementing a basic text generation application using Python and the OpenAI API. We will experiment with different prompts and parameters to generate diverse text outputs. Let's get started!\n",
        "\n",
        "---------\n",
        "\n",
        "### Accessing and using the ChatGPT API\n",
        "\n",
        "#### 1. Create an API key:\n",
        "To access the ChatGPT API, participants will need to sign up for an API key from the OpenAI platform (https://beta.openai.com/signup/). After signing up, they will receive an API key that will be used to authenticate their requests.\n",
        "\n",
        "#### 2. Install the necessary libraries:\n",
        "Participants will need to install the `requests` library to interact with the ChatGPT API using Python. They can install it using pip:"
      ],
      "metadata": {
        "id": "H7ZNpsPUZvMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uZ4vkhKaPP4",
        "outputId": "faaa95dd-6259-403f-98ab-971096784fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Set up the API request:\n",
        "Participants will use the following code snippet to send a request to the ChatGPT API. They should replace `YOUR_API_KEY` with their actual API key."
      ],
      "metadata": {
        "id": "JcNyGaHGaifz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def generate_response(prompt):\n",
        "    api_key = \"YOUR API KEY\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 100,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    response = requests.post(\"https://api.openai.com/v1/engines/text-davinci-002/completions\", headers=headers, json=data)\n",
        "    response_json = response.json()\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response_json[\"choices\"][0][\"text\"]\n",
        "    else:\n",
        "        return f\"Error: {response_json['error']['message']}\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "acFRj3QsafQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function, `generate_response`, which takes a user's prompt as input and sends it to the ChatGPT API. The response is then printed to the console. Now we are ready to run our first query!\n",
        "\n",
        "1) Ask a simple question (e.g. Tell a joke about <something>)\n",
        "\n",
        "2) Ask a question that generates a longer response or about a complex topic. What do you notice about the response?"
      ],
      "metadata": {
        "id": "_GmI1FCwg4rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Ask a question: \")\n",
        "response = generate_response(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eD-9HG5dQV3",
        "outputId": "d9055294-2f03-4c49-e1b0-1dad3d467999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question: hello\n",
            "Error: Incorrect API key provided: YOUR API KEY. You can find your API key at https://platform.openai.com/account/api-keys.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add a couple of more pieces of code so that we can have more nicely formatted code, and ask multiple questions of our system!"
      ],
      "metadata": {
        "id": "6eE-DrXGg7Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textwrap3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMJqorV3fVWi",
        "outputId": "bed43c3b-59ec-4c1e-c2ac-d796aff3ded0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textwrap3\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Installing collected packages: textwrap3\n",
            "Successfully installed textwrap3-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n"
      ],
      "metadata": {
        "id": "5INTP7DJfaX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = input(\"Ask a question: \")\n",
        "response = generate_response(prompt)\n",
        "\n",
        "wrapped_response = textwrap.fill(response, width=80)\n",
        "print(wrapped_response)"
      ],
      "metadata": {
        "id": "6zj7xdQnfAVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae2f62c-1e65-4313-8ba3-71e56aa66bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question: What is Deep Learning?\n",
            "  Deep learning is a subset of machine learning that is concerned with\n",
            "algorithms inspired by the structure and function of the brain called artificial\n",
            "neural networks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we wanted to make it so the user can continue to ask questions of the model to follow up? Let's try out the following While Loop that will continue to iterate through our code:\n",
        "\n",
        "1) Try asking something simple\n",
        "\n",
        "2) Try asking something more complex based on the previous response to check for continuity.\n",
        "\n",
        "3) Try to ask for some python code. What do you notice here vs the Web-version?"
      ],
      "metadata": {
        "id": "GymGrEjQf-s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    prompt = input(\"Ask a question (type 'exit' to quit): \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "    response = generate_response(prompt)\n",
        "    wrapped_response = textwrap.fill(response, width=80)\n",
        "    print(wrapped_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hee1qFpVf5r3",
        "outputId": "2e75f20b-4823-4e9b-c55a-c7b4780bda45"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (type 'exit' to quit): What are Large Language Models?\n",
            "  Large language models are statistical models that are trained on large amounts\n",
            "of data in order to learn the statistical properties of a language. These models\n",
            "can be used for a variety of tasks, such as language modeling, machine\n",
            "translation, and information retrieval.\n",
            "Ask a question (type 'exit' to quit): What is language modeling in the context of LLMs?\n",
            "  In the context of LLMs, language modeling is the process of predicting the\n",
            "probability of a sequence of words.\n",
            "Ask a question (type 'exit' to quit): How does it do this?\n",
            "  It creates a new window with the specified width, height, and other options.\n",
            "Ask a question (type 'exit' to quit): What does that mean?\n",
            "  A:  That means that the seller is willing to accept a lower price in order to\n",
            "sell the item more quickly.\n",
            "Ask a question (type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's add a layer of complexity by doing 2 things:\n",
        "\n",
        "1) Installing the OpenAI Package for Python\n",
        "\n",
        "2) Using davinci-003 text (GPT) model"
      ],
      "metadata": {
        "id": "VJ5w9pM8i6w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywbGQaeGiIv-",
        "outputId": "07909959-6863-40b2-fea7-058c77be670b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.2.4-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.2/220.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a moment to notice what's different about this code compared to the previous one.\n",
        "\n",
        "Check out more about the differences between the previous model and the new model we are going to try out here:\n",
        "\n",
        "[How do text-davinci-002 and text-davinci-003 differ?](https://help.openai.com/en/articles/6779149-how-do-text-davinci-002-and-text-davinci-003-differ)"
      ],
      "metadata": {
        "id": "28LoMi7gjJ2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import openai\n",
        "openai.api_key = 'YOUR API_KEY'\n",
        "def generate_text(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        temperature=0.6,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        echo=True\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "prompt = \"Once upon a time\"\n",
        "generated_text = generate_text(prompt)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gdUgfGcjeIwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we have the same while loop as before but we've changed the function name to our new function: `generate_text()`\n",
        "\n",
        "Try out the following:\n",
        "\n",
        "1) Ask a simple question\n",
        "\n",
        "2) Ask a more comprehensive question.\n",
        "\n",
        "3) What differences do you notice in your perception of the model? Especially if you asked the same questions?"
      ],
      "metadata": {
        "id": "4Zjymy-Rj0DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    prompt = input(\"Ask a question (type 'exit' to quit): \")\n",
        "    if prompt.lower() == 'exit':\n",
        "        break\n",
        "    response = generate_text(prompt)\n",
        "    wrapped_response = textwrap.fill(response, width=80)\n",
        "    print(wrapped_response)"
      ],
      "metadata": {
        "id": "FnzTe24YfUia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customizing the chatbot for specific use cases\n",
        "To customize the chatbot, participants can adjust the following parameters in the `generate_text()` function:\n",
        "\n",
        "#### 1. Temperature:\n",
        "Controls the randomness of the generated response. Higher values (e.g., 1.0) make the output more creative, while lower values (e.g., 0.2) make it more deterministic.\n",
        "\n",
        "#### 2. Max tokens:\n",
        "Controls the maximum length of the generated response. Participants can set it to a lower value to receive shorter answers or a higher value for longer explanations.\n",
        "\n",
        "#### 3. Top p:\n",
        "An alternative to temperature, this parameter filters the tokens generated by the model based on their cumulative probability. It can be used to control the diversity of the generated responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "hYCBZUCQg2k3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU TRY\n",
        "\n",
        "Try out a modified `response_text()` function and play with the following parameters to see how your responses change to THE SAME QUESTION (sufficiently complex is better):\n",
        "\n",
        "1) Temperature (`temperature`)\n",
        "\n",
        "2) Max Tokens (`max_tokens`) - be careful to not make this too large! ($$$)\n",
        "\n",
        "3) Top p (`top_p`)"
      ],
      "metadata": {
        "id": "YWS-aqJHkUCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE"
      ],
      "metadata": {
        "id": "i5DjWwhPKIci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLOSING THOUGHTS AND NEXT STEPS\n",
        "\n",
        "Congratulations on completing the Basic Text Generation Lab with ChatGPT\n",
        "\n",
        "You have gained hands-on experience with the OpenAI API and learned how to generate text using prompts. Feel free to experiment further with different prompts, parameters, and use cases.\n",
        "\n",
        "Generative AI opens up exciting possibilities for creative writing, content generation, and more. As you continue your journey, explore advanced features, experiment with diverse prompts, and try integrating ChatGPT into your own projects!\n",
        "\n",
        "Remember to document your code, experiments, and results, and share your progress with your peers. Generative AI is a rapidly evolving field, and your contributions can inspire and benefit others.\n",
        "\n",
        "Keep exploring, experimenting, and pushing the boundaries of what AI can do.\n",
        "\n",
        "**Happy generating!**\n",
        "\n",
        "\n",
        "REFERENCES:\n",
        "https://platform.openai.com/docs/models/continuous-model-upgrades\n",
        "\n",
        "https://platform.openai.com/docs/introduction\n",
        "\n",
        "https://platform.openai.com/docs/models\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uG6wIDeIk6Dy"
      }
    }
  ]
}