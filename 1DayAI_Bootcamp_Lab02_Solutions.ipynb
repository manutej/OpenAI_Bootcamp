{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Basic Chatbot\n",
        "\n",
        "Before we dive into the exciting world of bringing our very own ChatBot, we will want to review a few concepts and expand our knowledge on how to perform API calls, with the concept of **\"Message Threading\"**\n",
        "\n",
        "\n",
        "## How Message Threading Works\n",
        "Message threading is a conversation technique that allows multiple participants to communicate clearly within a linked series of messages. Here is an overview of how threaded messaging facilitates an organized dialogue:\n",
        "\n",
        "## Participants\n",
        "There are three main roles that contribute to a message thread conversation: System, User, and Assistant\n",
        "\n",
        "\n",
        "\n",
        "### The System\n",
        "Initiates the conversation by providing opening instructions and prompts.\n",
        "Guides the dialogue on the back-end by providing your instructions, which helps \"mold\" responses from the Assistant\n",
        "\n",
        "For example:\n",
        "\n",
        "**System**: You are a helpful assistant.\n",
        "\n",
        "**System**: You are a geography expert.\n",
        "\n",
        "### The User\n",
        "Drives the conversation based on prompts from the system.\n",
        "Asks questions, provides responses, and makes follow-up comments.\n",
        "\n",
        "For example:\n",
        "\n",
        "**User**: What is your name?\n",
        "\n",
        "**User**: What is the capital of the United States?\n",
        "\n",
        "The Assistant\n",
        "Responds directly to each of the user's messages.\n",
        "Answers questions and provides relevant information.\n",
        "Continues the conversation with the user naturally.\n",
        "\n",
        "For example:\n",
        "\n",
        "**Assistant**: I am ChatGPT developed by OpenAI, I am a helpful assistant here to answer your questions!\n",
        "\n",
        "**Assistant**: The capital of the United States of America is Washington D.C. ...\n"
      ],
      "metadata": {
        "id": "mcMLLWlUOWNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDB0VAL-OSlP",
        "outputId": "4a1268e4-ee84-440e-f03f-13c68f22482c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.2.4-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.2/220.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayi2cUzXOLCs"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-3.5-turbo-1106\"\n",
        "#model = \"gpt-4-1106-preview\"\n",
        "openai.api_key = \"YOUR API KEY\"\n"
      ],
      "metadata": {
        "id": "ns-Ao1wlPgXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = openai.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I output all files in a directory using Python?\",\n",
        "        },\n",
        "    ],\n",
        ")\n"
      ],
      "metadata": {
        "id": "CSQbtOElUCOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU TRY:\n",
        "\n",
        "Looks like we'll need to parse out the response. How can we do this in a way that helps us get the actual content of the message, without all the extra \"metadata\"?"
      ],
      "metadata": {
        "id": "v9kZI_PcQatr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3O-nyalHQaJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ANSWER\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4H7mVVvQlBj",
        "outputId": "451aa336-043a-40dc-84d8-b58364ef3586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can use the `os` module in Python to list all the files in a directory. Here's an example of how you can do it:\n",
            "\n",
            "```python\n",
            "import os\n",
            "\n",
            "directory = '/path/to/your/directory'\n",
            "\n",
            "# List all files in the directory\n",
            "files = os.listdir(directory)\n",
            "\n",
            "# Print the list of files\n",
            "for file in files:\n",
            "    print(file)\n",
            "```\n",
            "\n",
            "Replace `/path/to/your/directory` with the actual path of the directory you want to list the files for. This code will print the names of all the files in the directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU TRY:\n",
        "\n",
        "Now let's write a simple function that loads our model type, and our prompt, to generate a working output directly, so we can pass that function directly! We will be using the GPT 3.5 model (Turbo) which is specified as the parameter: `model = \"gpt-3.5-turbo-1106\"`"
      ],
      "metadata": {
        "id": "cH3vNPuRQ7ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MESSAGES = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I output all files in a directory using Python?\",\n",
        "        },\n",
        "    ]"
      ],
      "metadata": {
        "id": "6XTgvE-9RoKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "4mCUMkSfQ68U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SOLUTION\n",
        "#from openai.types import Completion, CompletionChoice, CompletionUsage\n",
        "\n",
        "def get_response(messages=MESSAGES, model = model):\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "    result = response.choices[0].message.content\n",
        "    return result"
      ],
      "metadata": {
        "id": "f5vy427gQ5pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_response(MESSAGES,model = \"gpt-3.5-turbo-1106\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCmTJfvPRJua",
        "outputId": "ada77ed7-82a9-43fe-a368-d0cf55f22bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The World Series in 2020 was played at a neutral site, Globe Life Field in Arlington, Texas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expected Response\n",
        "You can use the `os` module in Python to achieve this. Here's an example code to output all files in a directory:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "directory = '/path/to/your/directory'\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if os.path.isfile(os.path.join(directory, filename)):\n",
        "        print(filename)\n",
        "```\n",
        "\n",
        "Replace `'/path/to/your/directory'` with the actual path of the directory you want to list the files from. This code will iterate through all the files in the directory and print their names."
      ],
      "metadata": {
        "id": "sEHoKxZpXmRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try another example"
      ],
      "metadata": {
        "id": "nONMZQQVYAax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MESSAGES2=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who knows how to program.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "r67u_J9sX_Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_response(MESSAGES,model = \"gpt-3.5-turbo-1106\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3Ycv61yYDBM",
        "outputId": "6164df40-573e-44cf-a0f7-af253573422a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The World Series in 2020 was played at Globe Life Field in Arlington, Texas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Latest and Greatest\n",
        "\n",
        "At the time of this writing, the newest model that has been released is GPT4-turbo which as an advanced version of GPT-4 which has the following key features:\n",
        "\n",
        "- 128K context window\n",
        "\n",
        "- 0.3 US cents per 1000 tokens\n",
        "\n",
        "- Faster than GPT-4\n",
        "\n",
        "- Has a \"Vision\" model to see images\n",
        "\n",
        "In order to test out the GPT4-turbo (text) model, we just have to change the model flag in our `get_response()` function to be:\n",
        "\n",
        "`model = \"gpt-4-1106-preview\"`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f3bwuWe-OLqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOU TRY\n",
        "\n",
        "Try this out now with while trying to help write an email to your boss explaining why we need to consider why we need to push the deadline back. Try out the responses from both the gpt3.5 model and new gpt-4 model.\n",
        "\n"
      ],
      "metadata": {
        "id": "bsFeZOtiYm8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MESSAGES = [...]\n",
        "print(\"GPT 3.5 Response:\")\n",
        "\n",
        "# YOUR CODE HERE\n",
        "print(\"GPT 4 Turbo Response: \")\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "LLnvmcS3ZBr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SOLUTION\n",
        "MESSAGES = [\n",
        "    {\"role\":\"user\",\n",
        "     \"content\": \"Help me write a cordial and concise email to my boss explaining why we need to push back the product launch.\"}\n",
        "    ]\n",
        "\n",
        "print(\"GPT 3.5 Response:\")\n",
        "\n",
        "print(get_response(MESSAGES,model = \"gpt-3.5-turbo-1106\"))\n",
        "\n",
        "print(\"GPT 4 Turbo Response: \")\n",
        "print(get_response(MESSAGES,model = \"gpt-4-1106-preview\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbexjJuTZzVP",
        "outputId": "4c4c3699-4b33-429c-f120-19b070524464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT 3.5 Response:\n",
            "Subject: Request to Push Back Product Launch\n",
            "\n",
            "Dear [Boss's Name],\n",
            "\n",
            "I hope this email finds you well. \n",
            "\n",
            "After carefully assessing the current status of our product development, I believe it is in the best interest of the company to push back the product launch date. \n",
            "\n",
            "Due to unforeseen challenges in the production process, we need more time to ensure that the final product meets the quality standards and expectations of our customers. \n",
            "\n",
            "I understand the significance of sticking to the original timeline, but I believe that delaying the launch will ultimately result in a better product and a more successful launch.\n",
            "\n",
            "I am confident that with the extra time, we will be able to address any remaining issues and deliver a product that we can all be proud of.\n",
            "\n",
            "Thank you for your understanding and consideration.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "GPT 4 Turbo Response: \n",
            "Subject: Request to Reschedule Product Launch Date\n",
            "\n",
            "Dear [Boss's Name],\n",
            "\n",
            "I hope this message finds you well.\n",
            "\n",
            "I am writing to discuss the upcoming launch of our product [Product Name], currently scheduled for [Original Launch Date]. After a thorough review of our project timeline and necessary benchmarks, I believe it is in our best interest to consider postponing the launch date.\n",
            "\n",
            "Several reasons have prompted this recommendation. Firstly, [reason 1, e.g., \"Feedback from our beta testing indicates that certain key features require additional refinement to ensure they meet customer expectations\"]. Secondly, [reason 2, e.g., \"Our supply chain has experienced unexpected delays, impacting our inventory readiness\"]. Lastly, [reason 3, if necessary, e.g., \"We need extra time to strengthen our marketing campaign to maximize impact upon release\"].\n",
            "\n",
            "Although the team has been diligently working towards the original launch date, these unforeseen challenges warrant extra time to ensure we deliver a product that aligns with our brand's reputation for quality and excellence. To address these matters comprehensively, I propose a new launch date of [Proposed New Launch Date], which would give us [amount of time, e.g., \"an additional four weeks\"] to resolve current issues and prepare more effectively.\n",
            "\n",
            "I'd appreciate the opportunity to discuss this further and provide a more detailed overview of the situation, along with our proposed solutions and adjusted project timeline. I believe this adjustment will ensure a successful launch that we can all be proud of and that will resonate well with our customers and stakeholders.\n",
            "\n",
            "Please let me know a convenient time to meet, or if I can provide further information beforehand.\n",
            "\n",
            "Thank you for your understanding and support. I look forward to your guidance on how best to proceed.\n",
            "\n",
            "Warm regards,\n",
            "\n",
            "[Your Name]\n",
            "[Your Job Title]\n",
            "[Your Contact Information]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Flow\n",
        "The conversation flows smoothly through the interactions between the different roles:\n",
        "\n",
        "The **system** provides an opening prompt to initiate the dialogue.\n",
        "\n",
        "The **user** responds to the prompt, posing their first question or comment to the assistant. This starts a message thread.\n",
        "\n",
        "The **assistant** directly answers the user's message, keeping their response in the same thread.\n",
        "\n",
        "The **user** can then follow up with additional questions or comments related to the assistant's response, further continuing the thread.\n",
        "\n",
        "This **cycle repeats**, with the assistant replying to each of the user's messages to maintain a natural back-and-forth discussion.\n",
        "\n",
        "When needed, the **system** can provide another prompt to start a new thread on a different topic.\n",
        "Message Organization\n",
        "Threading keeps the various messages organized:\n",
        "\n",
        "Messages that are part of the same thread are grouped visually so they are easy to follow.\n",
        "Each thread stems from an initial prompt and contains a chain of relevant user and assistant replies.\n",
        "Shifting to a new prompt begins a new thread with its own distinct message chain separate from previous threads.\n",
        "This structure allows multiple threads to exist simultaneously without confusion."
      ],
      "metadata": {
        "id": "ZH9f-dAmihff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU TRY\n",
        "### System Messages\n",
        "This time, let's try the same prompt again, but we'll add a \"system messagee\" that will alow us to provide some more detailed instructions like:\n",
        "\"\"You are a wise negotiator and business leader who writes expert-level persuasive emails\""
      ],
      "metadata": {
        "id": "ZQuGhglIaFgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MESSAGES = [{\"role\":\"system\",\n",
        "             \"content\":\"You are a wise negotiator and business leader who writes expert level persuasive emails\"},\n",
        "    {\"role\":\"user\",\n",
        "     \"content\": \"Help me write a cordial and concise email to my boss explaining why we need to push back the product launch.\"}]\n",
        "print(\"GPT 3.5 Response:\")\n",
        "\n",
        "print(get_response(MESSAGES,model = \"gpt-3.5-turbo-1106\"))\n",
        "\n",
        "print(\"GPT 4 Turbo Response: \")\n",
        "print(get_response(MESSAGES,model = \"gpt-4-1106-preview\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nsb1lBQQYmUZ",
        "outputId": "cae01ab5-ccd4-4dec-f73d-aee10b1070c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT 3.5 Response:\n",
            "Subject: Proposal to Postpone Product Launch\n",
            "\n",
            "Dear [Boss's Name],\n",
            "\n",
            "I hope this email finds you well. After carefully evaluating all aspects of our upcoming product launch, I strongly believe that in order to ensure optimal success and meet the desired standards, it is in our best interest to postpone the launch date.\n",
            "\n",
            "Several unforeseen challenges have arisen during the final stages of development, which have impacted the product's quality and readiness for market release. Pushing back the launch will provide us with the necessary time to address these issues comprehensively, allowing for a more polished and impactful debut.\n",
            "\n",
            "I am confident that delaying the launch will ultimately contribute to the long-term success of the product and the company as a whole. I am prepared to provide a more detailed overview of the specific challenges and proposed solutions should you require further information.\n",
            "\n",
            "Thank you for your understanding and consideration of this matter. I am fully committed to ensuring the success of this product and look forward to your feedback regarding this recommended course of action.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "[Your Name]\n",
            "GPT 4 Turbo Response: \n",
            "Subject: Proposal for Revised Product Launch Timeline\n",
            "\n",
            "Dear [Boss's Name],\n",
            "\n",
            "I hope this email finds you well. I am reaching out to discuss our current timeline for the upcoming product launch and to propose a revised schedule that I believe will greatly benefit the project and our company.\n",
            "\n",
            "After a careful review of the latest developments and feedback from the product development team, I have concluded that pushing back the launch date is necessary to ensure we meet our high standards for quality and performance. Here are a few key factors that have informed this recommendation:\n",
            "\n",
            "1. Feedback Incorporation: We have received valuable input from our beta testers that necessitate additional tweaks to enhance user experience. Addressing these points now will likely prevent future issues that could negatively impact our brand reputation.\n",
            "\n",
            "2. Technical Refinements: The development team has identified a few technical aspects that require further refinement to guarantee the product's reliability and compliance with industry standards.\n",
            "\n",
            "3. Supply Chain Adjustments: Unforeseen delays in our supply chain have impacted the availability of critical components, thus affecting our production schedule. A revised timeline will allow us to navigate these challenges more effectively.\n",
            "\n",
            "4. Market Readiness: Shifting our launch date will provide an opportunity to better align with market trends and upcoming industry events, potentially improving our product's reception and success in the marketplace.\n",
            "\n",
            "To provide a concrete plan, I suggest we consider a [X-week/month] postponement which will afford us the time to address the aforementioned points thoroughly. Attached is a revised project timeline that outlines key milestones and deliverables within the proposed timeframe.\n",
            "\n",
            "I understand the implications a change in schedule might have on our marketing and sales strategies; therefore, I am prepared to collaborate closely with all relevant departments to minimize disruptions and communicate changes effectively.\n",
            "\n",
            "Let's aim to deliver a product that not only meets customers' expectations but exceeds them, fortifying our reputation as a leader in innovation and quality.\n",
            "\n",
            "I appreciate your understanding and would be glad to discuss this proposal in greater detail at your earliest convenience.\n",
            "\n",
            "Thank you for considering this request for a revised product launch timeline. I am confident that with a little more time, we will ensure the most successful outcome for our product and company.\n",
            "\n",
            "Warm regards,\n",
            "\n",
            "[Your Name]\n",
            "[Your Position]\n",
            "[Your Contact Information]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've done a couple of more detailed examples on how to access the OpenAI API with some of the latest models, we can try to think about how we would help make some more abstractions in order to help run this as a chatbot.\n",
        "\n",
        "Take a moment to think about it and reflect.\n",
        "\n",
        "### Reflect: What will we need to encode a chatbot that goes back and forth, that maintains context over time?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w_9hWNm6YVAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Reflections and ideas here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HxMphHtebIZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we want to do is probably think of the general conversation threading and think about \"actions\" for each of the THREE PLAYERS: System, User, Assistant.\n",
        "\n",
        "How can we do this?\n",
        "\n",
        "Well, as often happens in the world of coding...we can actually ask ChatGPT for some advice and work from there."
      ],
      "metadata": {
        "id": "zR8lqCGcbDrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "code_sample = \"\"\"MESSAGES = [{\"role\":\"system\",\n",
        "             \"content\":\"You are a wise negotiator and business leader who writes expert level persuasive emails\"},\n",
        "    {\"role\":\"user\",\n",
        "     \"content\": \"Help me write a cordial and concise email to my boss explaining why we need to push back the product launch.\"}]\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are a coding assistant and Python expert. I am working with a Generative AI API, and I would like to\n",
        "figure out how I can create an automated loop chatbot, that uses only the simplest tools possible\n",
        "to help extend a message thread that is shown below separated by 3 backticks.\n",
        "```python\n",
        "{code_sample}\n",
        "```\n",
        "\"\"\"\n",
        "MESSAGES = [{\"role\":\"system\",\n",
        "             \"content\":\"You are a programming expert\"},\n",
        "    {\"role\":\"user\",\n",
        "     \"content\": prompt}]\n",
        "# print(\"GPT 3.5 Response:\")\n",
        "\n",
        "# print(get_response(MESSAGES,model = \"gpt-3.5-turbo-1106\"))\n",
        "\n",
        "print(\"GPT 4 Turbo Response: \")\n",
        "print(get_response(MESSAGES,model = \"gpt-4-1106-preview\"))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXlQ33LYauRj",
        "outputId": "8280aad4-4921-488a-eb31-80f515b1d625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT 4 Turbo Response: \n",
            "Certainly, you can create a simple loop chatbot in Python that uses this Generative AI API to generate responses. I'll provide you with a basic structure of the code. In this example, I will assume that the API interaction is conducted via a function `generate_response`, which you'll have to replace with the actual API call method provided by your Generative AI service.\n",
            "\n",
            "Here's a simple example of a loop chatbot in Python:\n",
            "```python\n",
            "import json\n",
            "import requests\n",
            "\n",
            "# This is the function you'll need to replace with the actual API call.\n",
            "# It should take the context and return the AI's response.\n",
            "def generate_response(context):\n",
            "    # Simulating an API call with a static response for illustration purposes.\n",
            "    return \"This is a simulated response.\"\n",
            "\n",
            "# Initialize message thread.\n",
            "MESSAGES = [\n",
            "    {\"role\": \"system\",\n",
            "     \"content\": \"You are a wise negotiator and business leader who writes expert level persuasive emails\"},\n",
            "    {\"role\": \"user\",\n",
            "     \"content\": \"Help me write a cordial and concise email to my boss explaining why we need to push back the product launch.\"}\n",
            "]\n",
            "\n",
            "# Begin conversation loop.\n",
            "while True:\n",
            "    chat_input = input(\"You: \")\n",
            "    \n",
            "    if chat_input.lower() == \"quit\":\n",
            "        print(\"Exiting chat.\")\n",
            "        break\n",
            "    \n",
            "    # Append user input to messages.\n",
            "    MESSAGES.append({\"role\": \"user\", \"content\": chat_input})\n",
            "    \n",
            "    # Create a context from the messages.\n",
            "    context = \"\"\n",
            "    for message in MESSAGES:\n",
            "        role = message[\"role\"]\n",
            "        content = message[\"content\"]\n",
            "        context += f'{role}: {content}\\n'\n",
            "\n",
            "    # Call the generate_response function (or API call) with the context.\n",
            "    ai_response = generate_response(context)\n",
            "\n",
            "    # Append the AI's response to the messages.\n",
            "    MESSAGES.append({\"role\": \"ai\", \"content\": ai_response})\n",
            "    \n",
            "    print(\"AI:\", ai_response)\n",
            "\n",
            "# Exit the program.\n",
            "print(\"Chatbot session has ended.\")\n",
            "```\n",
            "\n",
            "Some important things to consider:\n",
            "- The `generate_response` function is a placeholder. You must implement it to include the specific instructions for interacting with your AI API, this includes handling authentication, headers, body, and making a `POST` request, interpreting the response, and more.\n",
            "- If the API requires a more complex structure of the input (like tokens, ids, etc.), you need to modify the `context` variable accordingly.\n",
            "- The loop continues until the user types `quit`. Depending on your use case, you might want to implement additional commands for the user to interact with the bot (like restarting the conversation, asking for AI help outside the context, etc.).\n",
            "- When the loop ends, the program will print a message indicating the session has ended, but you can also add some cleanup actions if necessary for your specific setup before the program exits.\n",
            "- Be sure to handle errors and timeouts that might occur during the API request robustly, this code doesn't include error handling for simplicity.\n",
            "\n",
            "Remember to replace the placeholder function with actual code that sends and receives data from the Generative AI API you are using. Each API provider typically has documentation with examples that you can follow to make sure you're forming your requests properly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOU TRY\n",
        "\n",
        "Try out some of the suggestions, if there is any valuable information presented, in order to think through the problem of how we might actually get a working chat thread that has the following requirements:\n",
        "\n",
        "- uses an initial input from the user\n",
        "- system instructions have a certain intent (if desired), but can be included in the MESSAGES variable directly\n",
        "- the conversation maintains the history and brings subsequent results based on the entire context history\n",
        "- continues the chat until the user says \"quit\"\n",
        "\n",
        "**HINT: it is probably not as complicated as the AI guessed, and you should focus on the principles required to build the loop rather than the cut-and-dry code presented by ChatGPT.**\n",
        "\n",
        "**HINT2: ChatGPT does not know its own documentation (and recent updates) in its training...probably.**"
      ],
      "metadata": {
        "id": "rv1t3IRhcgpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR SOLUTION HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uOMBW6GgcL_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### FIRST SOLUTION\n",
        "SYSTEM_MSG = \"You are a helpful assistant ready save the day!\"\n",
        "MESSAGES = [\n",
        "    {\"role\":\"system\", \"content\":SYSTEM_MSG},\n",
        "\n",
        "]\n",
        "\n",
        "## USER TURN\n",
        "user_input = \"\"\n",
        "while user_input != \"quit\":\n",
        "    user_input = input(\"How may I help you today?\")\n",
        "    MESSAGES.append({\"role\":\"user\", \"content\":user_input})\n",
        "\n",
        "    print(get_response(MESSAGES,model = \"gpt-3.5-turbo-1106\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlNtgaofdVme",
        "outputId": "8082732e-c76d-4e77-e2af-c1d2c7b02531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How may I help you today?quit\n",
            "If you need any help, feel free to ask. I'm here to assist you!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### SECOND SOLUTION\n",
        "SYSTEM_MSG = \"You are a helpful assistant ready save the day!\"\n",
        "MESSAGES = [\n",
        "    {\"role\":\"system\", \"content\":SYSTEM_MSG},\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "user_input = \"\"\n",
        "while user_input != \"quit\":\n",
        "    ## USER TURN\n",
        "    user_input = input(\"How may I help you today?\")\n",
        "    MESSAGES.append({\"role\":\"user\", \"content\":user_input})\n",
        "\n",
        "    ## ASSISTANT TURN\n",
        "    assistant_response = get_response(MESSAGES,model = \"gpt-3.5-turbo-1106\")\n",
        "    print(f\"Assitant: {assistant_response}\")\n",
        "    MESSAGES.append({\"role\":\"assistant\", \"content\":assistant_response})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asFQ9slPfAcJ",
        "outputId": "98592fae-260e-4905-f5cd-a10e7cb0ee7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How may I help you today?quit\n",
            "Assitant: If you have any more questions in the future or need assistance, feel free to ask. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You've Done it!\n",
        "\n",
        "There you go, you have now built a simple chat bot that:\n",
        "- Uses OpenAI API calls (with the latest updates)\n",
        "\n",
        "- Uses instruction tuned prompts as well as User prompts\n",
        "\n",
        "- Remembers CONTEXT in its responses"
      ],
      "metadata": {
        "id": "Z2HdshCMfxpo"
      }
    }
  ]
}